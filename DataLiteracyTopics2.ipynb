{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Topic Modelling of DL Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim import corpora, models, similarities\n",
    "import bleach\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ConditionalFreqDist, FreqDist\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import nltk\n",
    "import gexf as g\n",
    "import collections\n",
    "import os.path\n",
    "import re\n",
    "\n",
    "# Fix invalid display error\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding - Carlson et al\n",
      "adding - Fonticiaro Oehrli - 2016 - Why data literacy matters\n",
      "adding - blog - sod - defining dl\n",
      "adding - Teal et al\n",
      "adding - Determining Data Information Literacy Needs- A Study of Students\n",
      "adding - Erwin - 2015 - Data Literacy Real-World Learning Through Problem-Solving With Data Sets\n",
      "adding - Data-Pop Alliance - 2015 - Beyond Data Literacy Reinventing Community Engagement and Empowerment in the Age of Data\n",
      "adding - blog- Advancing Data Literacy\n",
      "adding - Martin - 2014 - What Is Data Literacy\n",
      "adding - Mandinach Gummer - 2013 - A Systemic View of Implementing Data Literacy in Educator Preparation\n",
      "adding - Koltay - 2015 - Data literacy in search of a name and identity\n",
      "adding - Anderson et al\n",
      "adding - Federer Lu Joubert - 2016 - Data literacy training needs of biomedical researchers\n"
     ]
    }
   ],
   "source": [
    "indirname = \"/home/paulus/docs/datalit/\"\n",
    "paperlist = []\n",
    "titlelist = []\n",
    "paperdict = dict()\n",
    "   \n",
    "for fn in os.listdir(indirname ):\n",
    "    if fn.endswith(\".txt\"):\n",
    "       \n",
    "        hdl = open(indirname + fn)\n",
    "        txt = hdl.read()\n",
    "        paperlist.append(txt)\n",
    "        \n",
    "        index_of_dot = fn.index('.')\n",
    "        fns = fn[:index_of_dot].replace(\",\",\"\")\n",
    "        print ('adding - ' + fns)\n",
    "        titlelist.append(fns)\n",
    "        paperdict[fns]=dict()\n",
    "        paperdict[fns][\"text\"] = txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# load nltk's English stopwords as variable called 'stopwords'\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# load nltk's SnowballStemmer as variabled 'stemmer'\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#not super pythonic, no, not at all.\n",
    "#use extend so it's a big flat list of vocab\n",
    "totalvocab_stemmed = []\n",
    "totalvocab_tokenized = []\n",
    "for i in paperlist:\n",
    "    i = i.decode('utf-8')\n",
    "    allwords_stemmed = tokenize_and_stem(i) #for each item in 'synopses', tokenize/stem\n",
    "    totalvocab_stemmed.extend(allwords_stemmed) #extend the 'totalvocab_stemmed' list\n",
    "    \n",
    "    allwords_tokenized = tokenize_only(i)\n",
    "    totalvocab_tokenized.extend(allwords_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 69153 items in vocab_frame\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)\n",
    "print ('there are ' + str(vocab_frame.shape[0]) + ' items in vocab_frame')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.64 s, sys: 76.6 ms, total: 3.72 s\n",
      "Wall time: 3.66 s\n",
      "(13, 2227)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#define vectorizer parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n",
    "                                 min_df=0.2, stop_words='english',\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n",
    "\n",
    "%time tfidf_matrix = tfidf_vectorizer.fit_transform(paperlist) #fit the vectorizer to synopses\n",
    "\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "#print terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 80.1 ms, sys: 0 ns, total: 80.1 ms\n",
      "Wall time: 79.5 ms\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_clusters = 4\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "\n",
    "%time km.fit(tfidf_matrix)\n",
    "\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['doc_cluster.pkl', 'doc_cluster.pkl_01.npy', 'doc_cluster.pkl_02.npy']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "#uncomment the below to save your model \n",
    "#since I've already run my model I am loading from the pickle\n",
    "\n",
    "joblib.dump(km,  'doc_cluster.pkl')\n",
    "\n",
    "#km = joblib.load('doc_cluster.pkl')\n",
    "#clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    7\n",
       "1    3\n",
       "0    2\n",
       "2    1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arts = { 'title': titlelist, 'text': paperlist, 'cluster': clusters }\n",
    "\n",
    "frame = pd.DataFrame(arts, index = [clusters] , columns = ['title', 'cluster'])\n",
    "frame['cluster'].value_counts() #number of films per cluster (clusters from 0 to 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "\n",
      "Cluster 0 words: teacher, decisions, mandinach, decisions, data-driven, et, al., data-driven, data-driven, course, state, parent, et, prepared, know, students, gummer, policy, s, west, training, privacy, performance, ethics, september,\n",
      "\n",
      "Cluster 0 titles:0    Mandinach Gummer - 2013 - A Systemic View of I...\n",
      "0                                       Anderson et al\n",
      "Name: title, dtype: object\n",
      "\n",
      "\n",
      "Cluster 1 words: 's, n't, social, participating, design, inclusion, stories, narrative, journals, big, contextualize, big, mentioned, able, term, humanities, individuals, revolution, journalistic, visualization, concepts, promote, aims, defined, definitions,\n",
      "\n",
      "Cluster 1 titles:1                             blog - sod - defining dl\n",
      "1    Data-Pop Alliance - 2015 - Beyond Data Literac...\n",
      "1                        blog- Advancing Data Literacy\n",
      "Name: title, dtype: object\n",
      "\n",
      "\n",
      "Cluster 2 words: workshops, training, instructors, software, lessons, materials, participating, surveys, taught, reproducibility, allow, domain, focus, learned, conducted, able, run, feedback, http, particularly, digitized, general, lifecycle, available, hands-on,\n",
      "\n",
      "Cluster 2 titles:Teal et al\n",
      "\n",
      "\n",
      "Cluster 3 words: librarians, libraries, information, management, faculty, data, s, projects, learned, standards, relevant, service, research, data, data, visualization, data, training, offer, metadata, authenticity, course, respondents, preservation, curation,\n",
      "\n",
      "Cluster 3 titles:3                                        Carlson et al\n",
      "3    Fonticiaro Oehrli - 2016 - Why data literacy m...\n",
      "3    Determining Data Information Literacy Needs- A...\n",
      "3    Erwin - 2015 - Data Literacy Real-World Learni...\n",
      "3                Martin - 2014 - What Is Data Literacy\n",
      "3    Koltay - 2015 - Data literacy in search of a n...\n",
      "3    Federer Lu Joubert - 2016 - Data literacy trai...\n",
      "Name: title, dtype: object\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "print()\n",
    "#sort cluster centers by proximity to centroid\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "\n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster %d words:\" % i, end='')\n",
    "    \n",
    "    for ind in order_centroids[i, :25]: #replace 6 with n words per cluster\n",
    "        print(' %s' % vocab_frame.ix[terms[ind].split(' ')].values.tolist()[0][0].encode('utf-8', 'ignore'), end=',')\n",
    "    print() #add whitespace\n",
    "    print() #add whitespace\n",
    "    \n",
    "    print(\"Cluster %d titles:\" % i, end='')\n",
    "    print(frame.ix[i]['title'])\n",
    "    #for title in frame.ix[i]['title'].value.tolist():\n",
    "        #print(' %s,' % title, end='')\n",
    "    print() #add whitespace\n",
    "    print() #add whitespace\n",
    "    \n",
    "print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Koltay - 2015 - Data literacy in search of a n...\n",
      "0                                       Anderson et al\n",
      "0    Data-Pop Alliance - 2015 - Beyond Data Literac...\n",
      "0                        blog- Advancing Data Literacy\n",
      "0    Mandinach Gummer - 2013 - A Systemic View of I...\n",
      "0    Determining Data Information Literacy Needs- A...\n",
      "0                                           Teal et al\n",
      "Name: title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print (frame.ix[i]['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
